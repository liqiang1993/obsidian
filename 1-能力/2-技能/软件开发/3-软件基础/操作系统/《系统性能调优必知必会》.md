# 基础设施优化

## CPU缓存：怎样写代码能够让CPU执行得更快？

### cpu缓存的分级

#### 单核独占

##### 一级缓存

##### 二级缓存

#### 多核心共享

##### 三级缓存

### 提高数据缓存命中率

#### cacheLine

##### 64字节

### 提高指令缓存命中率

#### 分支预测

### 提升多核 CPU 下的缓存命中率

#### 绑定cpu

#### 数据填充

## 内存池：如何提升内存分配的效率？

### 堆内存的分配顺序

#### 应用内存池

#### C库内存池

##### 每个子线程分配64M

##### 子线程数=8*核数

##### 分类

-   Ptmalloc2
    

-   均衡型
    

-   TCMalloc
    

-   适合多线程小内存
    

#### 操作系统内核

### 分配速度：栈>堆，可行的情况下，优先从栈分配

#### go 即使语义是从堆内存分配，也会尝试优化为从栈分配

## 索引：如何用哈希表管理亿级对象？

### 索引的基本思想就是以空间换时间

### hash表的时间复杂度是O（1）

### 内存结构与序列化方案

#### 哈希冲突的解决方法

##### 链接法

##### 开放寻址法

-   数据都在数组中，利于序列化
    

#### 把数据放在hash桶之外的原因

##### 数据占用内存较大

##### 空桶存在的话，可能存在内存浪费

##### 数据提出后， 哈希桶内单个元素占用的大小变低

#### 把数据从哈希桶提取出来的方法

##### 数据大小固定的情况

-   固定大小的数组
    
-   使用链表将待分配的数据管理起来
    

##### 数据大小不固定的情况

-   划分多个固定发小级别的数组
    
-   使用链表将待分配的数据管理起来
    

#### 降低hash冲突的方法

##### 增大hash桶的容量

##### 优化hash算法

-   除数因子选个素数
    

## 零拷贝：如何高效地传输文件？

### 原始读磁盘文件用网络发送的场景

#### 磁盘-》pagecache内存

#### pagecache内存-》用户内存

#### 用户内存-》socket缓存

#### socket缓存-》网卡

### 零拷贝技术

#### 优点

##### 减少用户态和内核态的上下文切换

##### 减少内存拷贝次数

#### 缺点

##### 基于pagecache实现， 对于大文件的场景不友好

#### 适用场景

##### 小文件的拷贝传输

-   阈值可配置
    

### pagecache

#### 优点

##### 预读取机制，对小文件的访问很友好

##### 整合写磁盘的机制， 写文件到磁盘的性能好

#### 缺点

##### 面对大文件时，预读取机制造成大文件缓存无效， 小文件入缓存失败

### 异步IO

#### 尽可能减少阻塞

### 同步IO

#### 直接 IO 是应用程序绕过 PageCache，即不经过内核缓冲区，直接访问磁盘中的数据，从而减少了内核缓存与用户程序之间的数据拷贝

#### 场景

##### 应用程序已经自己实现了磁盘文件的缓存，不需要 PageCache 再次进行缓存，引发额外的性能消耗

##### 高并发下传输大文件，因为大文件难以命中 PageCache 缓存，又会影响其他热点小文件的缓存

#### 缺点

##### 因为直接 IO 不适用 PageCache 缓存，所以享受不到内核针对 PageCache 做的一些优化

### 方法论

#### 大文件交给异步 IO 和直接 IO 处理，小文件交给零拷贝处理

### 文件传输的性能优化思路

#### 减少磁盘的工作量（PageCache 技术）

#### 提高内存的利用率（零拷贝）

#### 较少 CPU 的工作量（直接 IO）

## 协程：如何快速地实现高并发服务？

### 多进程

#### 缺点

##### 占用资源多， 切换成本高

#### 优点

##### 进程独立，稳定

### 多线程

#### 缺点

##### 切换成本仍然可观

##### 每个线程的栈内存8M，堆内存64M， 能够开启的总量有限

#### 优点

##### 相比于进程有进步

### 异步

#### 缺点

##### 违反了代码的内聚性，还需要业务代码关注并发细节，开发成本很高

#### 优点

##### 减少了每个请求的内存消耗，也降低了切换请求的成本

### 协程

#### 优点

##### 切换成本低，占用资源少， 支持高并发

##### 开发成本低

#### 缺点

##### 协程框架的普及度不足

#### 注意事项

##### 不能在协程中调用可能引起系统阻塞的函数

## 锁：如何根据业务场景选择合适的锁？

### 悲观锁

#### 互斥锁

##### 基础

##### 获取、释放锁的时间成本为几十纳秒到几微秒

##### 适合处理时间较长的业务

#### 自旋锁

##### 基础

##### 适合处理时间很短的场景， 微妙级

#### 读写锁

##### 读优先实现

##### 写优先实现

##### 公平实现

### 乐观锁

#### 先干活，后检测处理

#### 类似分支预测， 冲突概率很低的场景才可以用

# 系统网络优化

## 性能好，效率高的一对多通讯该如何实现？

### 广播

### 组播

### ip地址分类

### 网络号、主机号

### 子网掩码

### 广播地址

## 事件驱动：C10M是如何实现的？

### 网络事件产生的定义：一个网络报文到来，可能会产生一个时间，故一个http请求可能对应着多个事件

### tcp建立连接时会产生一读一写两个事件

### tcp关闭连接时会产生一个读事件

### 网络请求处理采用事件驱动的异步框架处理，性价比最高

### 多路复用

### epoll效率高的原因

#### 只用关注产生事件的连接

#### ，处理 epoll 收集到的事件时，必须保证处理一个事件的平均时间在毫秒级以内

## 如何提升TCP三次握手的性能？

### tcp_syn_retries控制客户端重发次数

### tcp_max_syn_backlog调整sync半连接的长度

### tcp_synack_retries控制服务器回复的次数

### accept队列满后的处置策略

### tcp_fastopen开启TFO， 减少一个RTT时间

### 三次握手的状态转换

#### SYNC_SENT

#### SYNC_RECEIVE

#### ESTABLISHED

## 如何提升TCP四次挥手的性能？

### 四次挥手的状态转换

#### FIN_WAIT1

#### CLOSE_WAIT

#### FIN_WAIT2

#### LAST_ACK

#### WAIT_CLOSE

#### CLOSING

### 关闭连接的方式

#### close

##### 孤儿连接

#### shutdown

### tcp_orphan_retries控制重发FIN的次数

### tcp_max_orphans 定义了孤儿连接的最大数量

#### 超过配置的数量后直接RST关闭连接

### tcp_fin_timeout控制孤儿连接存在的时长

### TIME_WAIT保留60秒的原因

#### 默认的两个MSL的时长， 单个MSL的时长是30S

#### 双向连接close后端口释放， 防止残存的FIN包误杀正常的连接

### tcp_max_tw_buckets配置处于TIME_WAIT状态的连接的数量， 超过配置值后的连接不再经历TIME_WAIT状态直接关闭

### tcp_tw_reuse配置可以让处于TIME_WAIT状态的端口作为请求方端口被复用

## 如何修改TCP缓冲区才能兼顾并发数量与传输速度？

### 滑动窗口是怎样影响传输速度的？

#### TCP 报文发出去后，并不能立刻从内存中删除，因为重发时还需要用到它

#### 发送方提速的方式很简单，并行地批量发送报文，再批量确认报文即可

#### 接收方把它的处理能力告诉发送方，使其限制发送速度即可，这就是滑动窗口的由来

### 带宽时延积如何确定最大传输速度？

### 由于发送缓冲区决定了发送窗口的上限，而发送窗口又决定了已发送但未确认的飞行报文的上限，因此，发送缓冲区不能超过带宽时延积，因为超出的部分没有办法用于有效的网络传输，且飞行字节大于带宽时延积还会导致丢包；而且，缓冲区也不能小于带宽时延积，否则无法发挥出高速网络的价值

### 怎样调整缓冲区去适配滑动窗口

#### 缓存区大小为什么不设定为带宽时延积？

##### 因为不是每个请求都能达到最大传输速度， 固定大小会浪费缓存

#### 缓冲区动态调节功能

##### tcp_wmem配置（单位字节）发送缓存区

-   发送缓冲区完全根据需求自行调整。比如，一旦发送出的数据被确认，而且没有新的数据要发送，就可以把发送缓冲区的内存释放掉
    
-   默认开启
    

##### tcp_rmem配置（单位字节）接收缓存区

-   依据空闲系统内存的数量来调节接收窗口
    
-   配置 tcp_moderate_rcvbuf 为 1 来开启调节功能
    
-   接收缓存区根据tcp_mem配置（单位是页面）来判断内存是否空闲
    
-   当 TCP 内存小于第 1 个值时，不需要进行自动调节；在第 1 和第 2 个值之间时，内核开始调节接收缓冲区的大小；大于第 3 个值时，内核不再为 TCP 分配新内存，此时新连接是无法建立的
    

#### 在高并发服务器中，为了兼顾网速与大量的并发连接，我们应当保证缓冲区的动态调整上限达到带宽时延积，而下限保持默认的 4K 不变即可。而对于内存紧张的服务而言，调低默认值是提高并发的有效手段

#### 如果这是网络 IO 型服务器，那么，调大 tcp_mem 的上限可以让 TCP 连接使用更多的系统内存，这有利于提升并发能力

#### 千万不要在 socket 上直接设置 SO_SNDBUF 或者 SO_RCVBUF，这样会关闭缓冲区的动态调整功能

## 如何调整TCP拥塞控制的性能？

### 拥塞控制的背景

#### 无拥塞控制时，发送方只有在重传RTO时间超时后才会发现报文被丢弃然后重传报文， 时间太长

### 拥塞窗口

#### 发送窗口<=min(拥塞窗口， 接收窗口)

#### 通常用 MSS 作为描述窗口大小的单位，其中 MSS 是 TCP 报文的最大长度。虽然窗口的计量单位是字节

### TCP慢启动

#### 拥塞窗口指数增长

#### 结束的三个场景

##### 通过定时器明确探测到了丢包

##### 拥塞窗口的增长到达了慢启动阈值 ssthresh

##### 接收到重复的 ACK 报文，可能存在丢包

### 拥塞窗口增长原理

#### 慢启动阶段指数增长

#### 拥塞避免阶段线性增长

### 拥塞控制算法

#### 慢启动

#### 拥塞避免

#### 快速重传

#### 快速恢复

## 实战：单机如何实现管理百万主机的心跳服务？

### 核心算法

#### 哈希表+有序链表+指针实现

### 高并发架构

#### 多核

#### 减少数据共享带来的损耗

#### 优化内存分配

### 选择合适的网络协议

#### 可靠性需求低， 数据小于MTU的， 考虑使用UDP

#### 可靠性需求高， 数据大于MTU的， 考虑使用TCP

# 应用层编解码优化

## 优化TLS/SSL性能该从何下手？

### 优化对称加密算法

#### DES已被攻破， 优先使用AES

#### 通常我们应选择可以并行计算的 GCM 分组模式，这也是当下互联网中最常见的 AES 分组算法

#### 如果 CPU 支持 AES-NI 特性，那么应选择 AES 算法，否则可以选择CHACHA20 对称加密算法，它主要使用 ARX 操作（add-rotate-xor），CPU 执行起来更快。

### 如何更快地协商出密钥？

#### 选择合适的算法

##### RSA 密钥协商算法

-   缺点是不支持前向保密，一旦服务器的私钥泄露，过去被攻击者截获的所有 TLS 通讯密文都会被破解
    

##### DH（Diffie–Hellman）密钥协商算法

-   解决了前向保密的问题
    

##### ECDH 密钥交换算法

-   ECDH 在 DH 算法的基础上利用ECC 椭圆曲线特性，可以用更少的计算量计算出公钥以及最终的密钥
    
-   解决DH算法性能不足的问题
    

#### 减少秘钥协商的次数

##### 长连接

##### session

##### session ticket

### 为什么应当尽快升级到 TLS1.3？

#### TLS1.3对性能的最大提升，在于它把 TLS 握手时间从 2 个 RTT 降为 1 个 RTT。

#### TLS1.3 仅支持目前最安全的几个算法

## 如何提升HTTP/1.1性能？

### http1.1的优缺点

#### 优点

##### 门槛低

##### 自表达

##### 易监控

#### 缺点

##### 默认使用短连接

##### 头部信息占用带宽比重大

##### 多次请求， 重复发送头信息， 信息冗余度高

### 优化http1的思路

#### 通过缓存避免发送 HTTP 请求

##### 浏览器缓存

-   缓存请求与响应体
    
-   缓存有超时时间
    
-   发送正式请求前发送摘要请求
    

##### 代理缓存

#### 必须发送时，尽量减少发送次数

##### 请求重定向优化处理

##### 合并请求

-   缺点是局部改动影响整体
    

##### 浏览器懒加载

#### 减少响应体的大小

##### 压缩

-   有损压缩
    
-   无损压缩
    

##### 压缩算法

-   gzip
    

-   使用范围广泛， 但压缩比和执行效率很一般
    

-   Brotli
    

-   压缩比和执行效率都优于gzip
    

## HTTP/2是怎样提升性能的？

### 静态表编码能节约多少带宽？

#### 静态表

#### huffman编码

### 动态表编码能节约多少带宽？

#### 动态表

##### 对同个连接中重复传递的内容才有效

#### 动态表会占用很多内存，影响进程的并发能力，所以服务器都会提供类似 http2_max_requests 这样的配置，限制一个连接上能够传输的请求数量，通过关闭 HTTP/2 连接来释放内存。因此，http2_max_requests 并不是越大越好，通常我们应当根据用户浏览页面时访问的对象数量来设定这个值。

### 如何并发传输请求？

#### stream流

### 服务器如何主动推送资源？

## Protobuf是如何进一步提高编码效率的？

### 怎样用最少的空间编码字段名？

#### 字段用客户端与服务端约定的数字代替

#### 字段名（数字）+类型 + 值长度 + 值

##### 字段名占用的字节范围1-5

### 怎样高效地编码字段值？

#### 6种值类型，6种编码方式

#### 字符串的编码是没有压缩的， 建议先压缩再编码

#### repeated 列表，使用打包功能可以仅用 1 个字段前缀描述所有数值，它在列表较大时能带来可观的空间收益

## 如何通过gRPC实现高效远程过程调用？

### 如何使用 gRPC 框架实现远程调用？

#### 定义Proto文件, 编译客户、服务端的基类

#### gRPC 消息是如何编码的？

#### 发起 RPC 调用后，我们可以这么分析抓取到的网络报文。首先，分析应用层最外层的 HTTP/2 帧，根据 Stream ID 找出一次 RPC 调用。客户端 HTTP 头部的 path 字段指明了 service 和 RPC 方法名，而 content-type 则指明了消息的编码格式。服务器端的 HTTP 头部被分成 2 次发送，其中 DATA 帧发送完毕后，才会发送 grpc-status 头部，这样可以明确最终的错误码。

#### gRPC 流模式的协议编码分析包体时，可以通过 Stream 中 Length-Prefixed Message 头部，确认 DATA 帧中含有多少个消息，因此可以确定这是一元模式还是流式调用。在 Length-Prefixed Message 头部后，则是 Protobuf 消息